# 3ã€å¤§æ¨¡å‹éƒ¨ç½²ä¸åŠ é€ŸæŒ‡å—

æœ¬ç« å†…å®¹å‚è€ƒDataWhaleçš„[llm-universe](https://github.com/datawhalechina/llm-universe)ï¼Œåœ¨æ­¤ç‰¹åˆ«æ„Ÿè°¢ï¼Œå¸Œæœ›å¤§å®¶å¤šå¤šæ”¯æŒè¿™ä¸ªé¡¹ç›®ã€‚

## 3.1. å¤§æ¨¡å‹éƒ¨ç½²è°ƒç”¨(ä»¥Qwen1.5ä¸ºä¾‹)

> Qwen1.5 æ˜¯ Qwen2 çš„æµ‹è¯•ç‰ˆï¼ŒQwen1.5 æ˜¯åŸºäº transformer çš„ decoder-only è¯­è¨€æ¨¡å‹ï¼Œå·²åœ¨å¤§é‡æ•°æ®ä¸Šè¿›è¡Œäº†é¢„è®­ç»ƒã€‚ä¸ä¹‹å‰å‘å¸ƒçš„ Qwen ç›¸æ¯”ï¼ŒQwen1.5 çš„æ”¹è¿›åŒ…æ‹¬ 6 ç§æ¨¡å‹å¤§å°ï¼ŒåŒ…æ‹¬ 0.5Bã€1.8Bã€4Bã€7Bã€14B å’Œ 72Bï¼›Chatæ¨¡å‹åœ¨äººç±»åå¥½æ–¹é¢çš„æ€§èƒ½æ˜¾è‘—æé«˜ï¼›åŸºç¡€æ¨¡å‹å’ŒèŠå¤©æ¨¡å‹å‡æ”¯æŒå¤šç§è¯­è¨€ï¼›æ‰€æœ‰å¤§å°çš„æ¨¡å‹å‡ç¨³å®šæ”¯æŒ 32K ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œæ— éœ€ trust_remote_codeã€‚

### 3.1.1 FastApiéƒ¨ç½²è°ƒç”¨

#### 3.1.1.1 ç¯å¢ƒå‡†å¤‡

åœ¨ Autodl å¹³å°ä¸­ç§Ÿèµä¸€ä¸ª 3090 ç­‰ 24G æ˜¾å­˜çš„æ˜¾å¡æœºå™¨ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºé•œåƒé€‰æ‹© PyTorch-->2.0.0-->3.8(ubuntu20.04)-->11.8ï¼ˆ11.3 ç‰ˆæœ¬ä»¥ä¸Šçš„éƒ½å¯ä»¥ï¼‰ã€‚
æ¥ä¸‹æ¥æ‰“å¼€åˆšåˆšç§Ÿç”¨æœåŠ¡å™¨çš„ JupyterLabï¼Œå¹¶ä¸”æ‰“å¼€å…¶ä¸­çš„ç»ˆç«¯å¼€å§‹ç¯å¢ƒé…ç½®ã€æ¨¡å‹ä¸‹è½½å’Œè¿è¡Œæ¼”ç¤ºã€‚
pipæ¢æºåŠ é€Ÿä¸‹è½½å¹¶å®‰è£…éœ€è¦çš„ä¾èµ–åŒ…

```shell
# å‡çº§pip
pythhon -m pip install --upgrade pip
# æ›´æ¢pipæºåŠ é€Ÿä¸‹è½½
pip  config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple

# å®‰è£…ä¾èµ–åŒ…
pip install fastapi== 0.104.1
pip install uvicorn==0.24.0.post1
pip install requests==2.25.1
pip install modelscope==1.11.0
pip install transformers==4.37.0
pip install streamlit==1.24.0
pip install sentencepiece==0.1.99
pip install accelerate==0.24.1
pip install transformers_stream_generator==0.0.4
```

#### 3.1.1.2 æ¨¡å‹ä¸‹è½½

ä½¿ç”¨modelscopeä¸­çš„snapshot_downloadä¸‹è½½æ¨¡å‹ï¼Œç¬¬ä¸€ä¸ªå‚æ•°ä¸ºæ¨¡å‹åç§°ï¼Œå‚æ•°cache_dirä¸ºæ¨¡å‹çš„ä¸‹è½½è·¯å¾„ã€‚
åœ¨æ–‡ä»¶ä¸‹æ–°å»ºmodel_download.pyæ–‡ä»¶ï¼Œå¹¶å†™å…¥ä¸€ä¸‹ä»£ç ã€‚åœ¨ç»ˆç«¯æ‰§è¡Œpython model_download.pyå³å¯ä¸‹è½½æ¨¡å‹ã€‚

```python
import torch 
from modelscope import snapshot_download, AutoModel, AutoTokenizer
import os
model_dir = snapshot_download('qwen/Qwen-1.5B-Chat',cache_dir = '/root.auto-tmp',revision='master')
```

#### 3.1.1.3 ä»£ç å‡†å¤‡

åœ¨ /root/autodl-tmp è·¯å¾„ä¸‹æ–°å»º api.py æ–‡ä»¶å¹¶åœ¨å…¶ä¸­è¾“å…¥ä»¥ä¸‹å†…å®¹ï¼Œç²˜è´´ä»£ç åè¯·åŠæ—¶ä¿å­˜æ–‡ä»¶ã€‚ä¸‹é¢çš„ä»£ç æœ‰å¾ˆè¯¦ç»†çš„æ³¨é‡Šï¼Œå¤§å®¶å¦‚æœ‰ä¸ç†è§£çš„åœ°æ–¹ï¼Œæ¬¢è¿æå‡º issueã€‚

```python
from fastapi import FastAPI, Request
from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig
import uvicorn
import json
import datetime
import torch

# è®¾ç½®è®¾å¤‡å‚æ•°
DEVICE = "cuda"  # ä½¿ç”¨CUDA
DEVICE_ID = "0"  # CUDAè®¾å¤‡IDï¼Œå¦‚æœæœªè®¾ç½®åˆ™ä¸ºç©º
CUDA_DEVICE = f"{DEVICE}:{DEVICE_ID}" if DEVICE_ID else DEVICE  # ç»„åˆCUDAè®¾å¤‡ä¿¡æ¯

# æ¸…ç†GPUå†…å­˜å‡½æ•°
def torch_gc():
    if torch.cuda.is_available():  # æ£€æŸ¥æ˜¯å¦å¯ç”¨CUDA
        with torch.cuda.device(CUDA_DEVICE):  # æŒ‡å®šCUDAè®¾å¤‡
            torch.cuda.empty_cache()  # æ¸…ç©ºCUDAç¼“å­˜
            torch.cuda.ipc_collect()  # æ”¶é›†CUDAå†…å­˜ç¢ç‰‡

# åˆ›å»ºFastAPIåº”ç”¨
app = FastAPI()

# å¤„ç†POSTè¯·æ±‚çš„ç«¯ç‚¹
@app.post("/")
async def create_item(request: Request):
    global model, tokenizer  # å£°æ˜å…¨å±€å˜é‡ä»¥ä¾¿åœ¨å‡½æ•°å†…éƒ¨ä½¿ç”¨æ¨¡å‹å’Œåˆ†è¯å™¨
    json_post_raw = await request.json()  # è·å–POSTè¯·æ±‚çš„JSONæ•°æ®
    json_post = json.dumps(json_post_raw)  # å°†JSONæ•°æ®è½¬æ¢ä¸ºå­—ç¬¦ä¸²
    json_post_list = json.loads(json_post)  # å°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºPythonå¯¹è±¡
    prompt = json_post_list.get('prompt')  # è·å–è¯·æ±‚ä¸­çš„æç¤º

    messages = [
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": prompt}
    ]

    # è°ƒç”¨æ¨¡å‹è¿›è¡Œå¯¹è¯ç”Ÿæˆ
    input_ids = tokenizer.apply_chat_template(messages,tokenize=False,add_generation_prompt=True)
    model_inputs = tokenizer([input_ids], return_tensors="pt").to('cuda')
    generated_ids = model.generate(model_inputs.input_ids,max_new_tokens=512)
    generated_ids = [
        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
    ]
    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
    now = datetime.datetime.now()  # è·å–å½“å‰æ—¶é—´
    time = now.strftime("%Y-%m-%d %H:%M:%S")  # æ ¼å¼åŒ–æ—¶é—´ä¸ºå­—ç¬¦ä¸²
    # æ„å»ºå“åº”JSON
    answer = {
        "response": response,
        "status": 200,
        "time": time
    }
    # æ„å»ºæ—¥å¿—ä¿¡æ¯
    log = "[" + time + "] " + '", prompt:"' + prompt + '", response:"' + repr(response) + '"'
    print(log)  # æ‰“å°æ—¥å¿—
    torch_gc()  # æ‰§è¡ŒGPUå†…å­˜æ¸…ç†
    return answer  # è¿”å›å“åº”

# ä¸»å‡½æ•°å…¥å£
if __name__ == '__main__':
    # åŠ è½½é¢„è®­ç»ƒçš„åˆ†è¯å™¨å’Œæ¨¡å‹
    model_name_or_path = '/root/autodl-tmp/qwen/Qwen1.5-7B-Chat'
    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=False)
    model = AutoModelForCausalLM.from_pretrained(model_name_or_path, device_map="auto", torch_dtype=torch.bfloat16)

    # å¯åŠ¨FastAPIåº”ç”¨
    # ç”¨6006ç«¯å£å¯ä»¥å°†autodlçš„ç«¯å£æ˜ å°„åˆ°æœ¬åœ°ï¼Œä»è€Œåœ¨æœ¬åœ°ä½¿ç”¨api
    uvicorn.run(app, host='0.0.0.0', port=6006, workers=1)  # åœ¨æŒ‡å®šç«¯å£å’Œä¸»æœºä¸Šå¯åŠ¨åº”ç”¨
```

#### 3.1.1.4 Apiéƒ¨ç½²

åœ¨ç»ˆç«¯æ‰§è¡Œä»¥ä¸‹å‘½ä»¤å¯åŠ¨FastAPIåº”ç”¨ï¼š

```shell
cd /root/autodl-tmp
python api.py
```

é»˜è®¤éƒ¨ç½²åœ¨ 6006 ç«¯å£ï¼Œé€šè¿‡ POST æ–¹æ³•è¿›è¡Œè°ƒç”¨ï¼Œå¯ä»¥ä½¿ç”¨ curl è°ƒç”¨ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```shell
curl -X POST "http://127.0.0.1:6006" \
     -H 'Content-Type: application/json' \
     -d '{"prompt": "ä½ å¥½"}'
```

ä¹Ÿå¯ä»¥ä½¿ç”¨ python ä¸­çš„ requests åº“è¿›è¡Œè°ƒç”¨ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```python
import requests
import json

def get_completion(prompt):
    headers = {'Content-Type': 'application/json'}
    data = {"prompt": prompt}
    response = requests.post(url='http://127.0.0.1:6006', headers=headers, data=json.dumps(data))
    return response.json()['response']

if __name__ == '__main__':
    print(get_completion('ä½ å¥½'))
```

å¾—åˆ°çš„è¿”å›å€¼å¦‚ä¸‹æ‰€ç¤ºï¼š

```json
{"response":"ä½ å¥½ï¼æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ","status":200,"time":"2024-02-05 18:08:19"}
```

### 3.1.2 WebDemoéƒ¨ç½²

#### 3.1.2.1 ç¯å¢ƒå‡†å¤‡

åœ¨autodlå¹³å°ä¸­ç§Ÿä¸€ä¸ª3090ç­‰24Gæ˜¾å­˜çš„æ˜¾å¡æœºå™¨ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºé•œåƒé€‰æ‹©PyTorch-->2.0.0-->3.8(ubuntu20.04)-->11.8ï¼ˆ11.3ç‰ˆæœ¬ä»¥ä¸Šçš„éƒ½å¯ä»¥ï¼‰
æ¥ä¸‹æ¥æ‰“å¼€åˆšåˆšç§Ÿç”¨æœåŠ¡å™¨çš„JupyterLabï¼Œ å›¾åƒ å¹¶ä¸”æ‰“å¼€å…¶ä¸­çš„ç»ˆç«¯å¼€å§‹ç¯å¢ƒé…ç½®ã€æ¨¡å‹ä¸‹è½½å’Œè¿è¡Œæ¼”ç¤ºã€‚

pipæ¢æºå’Œå®‰è£…ä¾èµ–åŒ…

```
# å‡çº§pip
python -m pip install --upgrade pip
# æ›´æ¢ pypi æºåŠ é€Ÿåº“çš„å®‰è£…
pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple

pip install modelscope==1.9.5
pip install "transformers>=4.37.0"
pip install streamlit==1.24.0
pip install sentencepiece==0.1.99
pip install accelerate==0.24.1
pip install transformers_stream_generator==0.0.4
```

#### 3.1.2.2 æ¨¡å‹ä¸‹è½½

ä½¿ç”¨ modelscope ä¸­çš„snapshot_downloadå‡½æ•°ä¸‹è½½æ¨¡å‹ï¼Œç¬¬ä¸€ä¸ªå‚æ•°ä¸ºæ¨¡å‹åç§°ï¼Œå‚æ•°cache_dirä¸ºæ¨¡å‹çš„ä¸‹è½½è·¯å¾„ã€‚

åœ¨ /root/autodl-tmp è·¯å¾„ä¸‹æ–°å»º download.py æ–‡ä»¶å¹¶åœ¨å…¶ä¸­è¾“å…¥ä»¥ä¸‹å†…å®¹ï¼Œç²˜è´´ä»£ç åè®°å¾—ä¿å­˜æ–‡ä»¶ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚å¹¶è¿è¡Œ python /root/autodl-tmp/download.py æ‰§è¡Œä¸‹è½½ï¼Œä¸‹è½½æ¨¡å‹å¤§æ¦‚éœ€è¦ 2 åˆ†é’Ÿã€‚

```python
import torch
from modelscope import snapshot_download, AutoModel, AutoTokenizer
from modelscope import GenerationConfig
model_dir = snapshot_download('qwen/Qwen1.5-7B-Chat', cache_dir='/root/autodl-tmp', revision='master')
```

#### 3.1.2.3 ä»£ç å‡†å¤‡

åœ¨ `/root/autodl-tmp`è·¯å¾„ä¸‹æ–°å»º `chatBot.py` æ–‡ä»¶å¹¶åœ¨å…¶ä¸­è¾“å…¥ä»¥ä¸‹å†…å®¹ï¼Œç²˜è´´ä»£ç åè®°å¾—ä¿å­˜æ–‡ä»¶ã€‚

```python
# å¯¼å…¥æ‰€éœ€çš„åº“
from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig
import torch
import streamlit as st

# åœ¨ä¾§è¾¹æ ä¸­åˆ›å»ºä¸€ä¸ªæ ‡é¢˜å’Œä¸€ä¸ªé“¾æ¥
with st.sidebar:
    st.markdown("## Qwen1.5 LLM")
    "[å¼€æºå¤§æ¨¡å‹é£Ÿç”¨æŒ‡å— self-llm](https://github.com/datawhalechina/self-llm.git)"
    # åˆ›å»ºä¸€ä¸ªæ»‘å—ï¼Œç”¨äºé€‰æ‹©æœ€å¤§é•¿åº¦ï¼ŒèŒƒå›´åœ¨0åˆ°1024ä¹‹é—´ï¼Œé»˜è®¤å€¼ä¸º512
    max_length = st.slider("max_length", 0, 1024, 512, step=1)

# åˆ›å»ºä¸€ä¸ªæ ‡é¢˜å’Œä¸€ä¸ªå‰¯æ ‡é¢˜
st.title("ğŸ’¬ Qwen1.5 Chatbot")
st.caption("ğŸš€ A streamlit chatbot powered by Self-LLM")

# å®šä¹‰æ¨¡å‹è·¯å¾„
mode_name_or_path = '/root/autodl-tmp/qwen/Qwen1.5-7B-Chat'

# å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºè·å–æ¨¡å‹å’Œtokenizer
@st.cache_resource
def get_model():
    # ä»é¢„è®­ç»ƒçš„æ¨¡å‹ä¸­è·å–tokenizer
    tokenizer = AutoTokenizer.from_pretrained(mode_name_or_path, use_fast=False)
    # ä»é¢„è®­ç»ƒçš„æ¨¡å‹ä¸­è·å–æ¨¡å‹ï¼Œå¹¶è®¾ç½®æ¨¡å‹å‚æ•°
    model = AutoModelForCausalLM.from_pretrained(mode_name_or_path, torch_dtype=torch.bfloat16,  device_map="auto")
  
    return tokenizer, model

# åŠ è½½Qwen1.5-4B-Chatçš„modelå’Œtokenizer
tokenizer, model = get_model()

# å¦‚æœsession_stateä¸­æ²¡æœ‰"messages"ï¼Œåˆ™åˆ›å»ºä¸€ä¸ªåŒ…å«é»˜è®¤æ¶ˆæ¯çš„åˆ—è¡¨
if "messages" not in st.session_state:
    st.session_state["messages"] = [{"role": "assistant", "content": "æœ‰ä»€ä¹ˆå¯ä»¥å¸®æ‚¨çš„ï¼Ÿ"}]

# éå†session_stateä¸­çš„æ‰€æœ‰æ¶ˆæ¯ï¼Œå¹¶æ˜¾ç¤ºåœ¨èŠå¤©ç•Œé¢ä¸Š
for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

# å¦‚æœç”¨æˆ·åœ¨èŠå¤©è¾“å…¥æ¡†ä¸­è¾“å…¥äº†å†…å®¹ï¼Œåˆ™æ‰§è¡Œä»¥ä¸‹æ“ä½œ
if prompt := st.chat_input():
    # å°†ç”¨æˆ·çš„è¾“å…¥æ·»åŠ åˆ°session_stateä¸­çš„messagesåˆ—è¡¨ä¸­
    st.session_state.messages.append({"role": "user", "content": prompt})
    # åœ¨èŠå¤©ç•Œé¢ä¸Šæ˜¾ç¤ºç”¨æˆ·çš„è¾“å…¥
    st.chat_message("user").write(prompt)
  
    # æ„å»ºè¾“å…¥   
    input_ids = tokenizer.apply_chat_template(st.session_state.messages,tokenize=False,add_generation_prompt=True)
    model_inputs = tokenizer([input_ids], return_tensors="pt").to('cuda')
    generated_ids = model.generate(model_inputs.input_ids, max_new_tokens=512)
    generated_ids = [
        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
    ]
    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
    # å°†æ¨¡å‹çš„è¾“å‡ºæ·»åŠ åˆ°session_stateä¸­çš„messagesåˆ—è¡¨ä¸­
    st.session_state.messages.append({"role": "assistant", "content": response})
    # åœ¨èŠå¤©ç•Œé¢ä¸Šæ˜¾ç¤ºæ¨¡å‹çš„è¾“å‡º
    st.chat_message("assistant").write(response)
    # print(st.session_state)
```

#### 3.1.2.4 è¿è¡Œdemo

åœ¨ç»ˆç«¯ä¸­è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼Œå¯åŠ¨streamlit	æœåŠ¡ï¼Œå¹¶æŒ‰ç…§ `autodl` çš„æŒ‡ç¤ºå°†ç«¯å£æ˜ å°„åˆ°æœ¬åœ°ï¼Œç„¶ååœ¨æµè§ˆå™¨ä¸­æ‰“å¼€é“¾æ¥ http://localhost:6006/ ï¼Œå³å¯çœ‹åˆ°èŠå¤©ç•Œé¢ã€‚

```bash
streamlit run /root/autodl-tmp/chatBot.py --server.address 127.0.0.1 --server.port 6006
```

å¦‚ä¸‹æ‰€ç¤ºï¼š
![Alt text](../pic/chatBot.png)

## 3.2 å¤§æ¨¡å‹åŠ é€Ÿ
### 3.2.1 åŸºäºllama.cpp
#### 3.2.1.1 è·å–å’Œç¼–è¯‘llama.cpp

##### 3.2.1.1.1 å…‹éš†llama.cppä»“åº“

```shell
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp
```

##### 3.2.1.1.2 ç¼–è¯‘llama.cpp

åœ¨llama.cppç›®å½•ä¸‹æ‰§è¡Œä»¥ä¸‹å‘½ä»¤ï¼š

```shell
make
```

#### 3.2.1.2  ä¸‹è½½qwenæ¨¡å‹

##### 3.2.1.2.1 å®‰è£…lfs

```shell
curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash
sudo apt-get install git-lfs
git lfs install
```

##### 3.2.1.2.2 æ¨¡å‹ä¸‹è½½

æ–°å»ºmodel_download.pyæ–‡ä»¶å¹¶åœ¨æ–‡ä»¶å†…è¾“å…¥ä»¥ä¸‹å†…å®¹ï¼š

```python
from modelscope.hub.file_download import model_file_download
 
model_dir = model_file_download(model_id='qwen/Qwen2-7B-Instruct-GGUF',
                                file_path='qwen2-7b-instruct-q5_k_m.gguf',
                                revision='master',
                                cache_dir='/root/autodl-tmp')

```

æ‰§è¡Œpython model_download.py

#### 3.2.1.3 ä½¿ç”¨llama.cppè¿›è¡Œæ¨ç†

```shell
./llama-cli -m /root/autodl-tmp/qwen/Qwen2-7B-Instruct-GGUF/qwen2-7b-instruct-q5_k_m.gguf \
  -n 512 -co -i -if -f prompts/chat-with-qwen.txt \
  --in-prefix "<|im_start|>user\n" \
  --in-suffix "<|im_end|>\n<|im_start|>assistant\n" \
  -ngl 24 -fa
```

<div align="center">
<img src='../pic/llama_cpp_chat.png'>
</div>


### 3.2.2 åŸºäºvLLM

 **vLLM ç®€ä»‹**

`vLLM` æ¡†æ¶æ˜¯ä¸€ä¸ªé«˜æ•ˆçš„å¤§è¯­è¨€æ¨¡å‹**æ¨ç†å’Œéƒ¨ç½²æœåŠ¡ç³»ç»Ÿ**ï¼Œå…·å¤‡ä»¥ä¸‹ç‰¹æ€§ï¼š

- **é«˜æ•ˆçš„å†…å­˜ç®¡ç†**ï¼šé€šè¿‡ `PagedAttention` ç®—æ³•ï¼Œ`vLLM` å®ç°äº†å¯¹ `KV` ç¼“å­˜çš„é«˜æ•ˆç®¡ç†ï¼Œå‡å°‘äº†å†…å­˜æµªè´¹ï¼Œä¼˜åŒ–äº†æ¨¡å‹çš„è¿è¡Œæ•ˆç‡ã€‚
- **é«˜ååé‡**ï¼š`vLLM` æ”¯æŒå¼‚æ­¥å¤„ç†å’Œè¿ç»­æ‰¹å¤„ç†è¯·æ±‚ï¼Œæ˜¾è‘—æé«˜äº†æ¨¡å‹æ¨ç†çš„ååé‡ï¼ŒåŠ é€Ÿäº†æ–‡æœ¬ç”Ÿæˆå’Œå¤„ç†é€Ÿåº¦ã€‚
- **æ˜“ç”¨æ€§**ï¼š`vLLM` ä¸ `HuggingFace` æ¨¡å‹æ— ç¼é›†æˆï¼Œæ”¯æŒå¤šç§æµè¡Œçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œç®€åŒ–äº†æ¨¡å‹éƒ¨ç½²å’Œæ¨ç†çš„è¿‡ç¨‹ã€‚å…¼å®¹ `OpenAI` çš„ `API` æœåŠ¡å™¨ã€‚
- **åˆ†å¸ƒå¼æ¨ç†**ï¼šæ¡†æ¶æ”¯æŒåœ¨å¤š `GPU` ç¯å¢ƒä¸­è¿›è¡Œåˆ†å¸ƒå¼æ¨ç†ï¼Œé€šè¿‡æ¨¡å‹å¹¶è¡Œç­–ç•¥å’Œé«˜æ•ˆçš„æ•°æ®é€šä¿¡ï¼Œæå‡äº†å¤„ç†å¤§å‹æ¨¡å‹çš„èƒ½åŠ›ã€‚
- **å¼€æºå…±äº«**ï¼š`vLLM` ç”±äºå…¶å¼€æºçš„å±æ€§ï¼Œæ‹¥æœ‰æ´»è·ƒçš„ç¤¾åŒºæ”¯æŒï¼Œè¿™ä¹Ÿä¾¿äºå¼€å‘è€…è´¡çŒ®å’Œæ”¹è¿›ï¼Œå…±åŒæ¨åŠ¨æŠ€æœ¯å‘å±•ã€‚

#### 3.2.2.1 ç¯å¢ƒå‡†å¤‡

`pip` æ¢æºåŠ é€Ÿä¸‹è½½å¹¶å®‰è£…ä¾èµ–åŒ…

```bash
# å‡çº§pip
python -m pip install --upgrade pip
# æ›´æ¢ pypi æºåŠ é€Ÿåº“çš„å®‰è£…
pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple
pip install modelscope==1.11.0
pip install openai==1.17.1
pip install torch==2.1.2+cu121
pip install tqdm==4.64.1
pip install transformers==4.39.3
# ä¸‹è½½flash-attn è¯·ç­‰å¾…å¤§çº¦10åˆ†é’Ÿå·¦å³~
MAX_JOBS=8 pip install flash-attn --no-build-isolation
pip install vllm==0.4.0.post1
```

ç›´æ¥å®‰è£… `vLLM` ä¼šå®‰è£… `CUDA 12.1` ç‰ˆæœ¬ã€‚

```bash
pip install vllm
```

#### 3.2.2.2 æ¨¡å‹ä¸‹è½½

ä½¿ç”¨ `modelscope` ä¸­çš„ `snapshot_download` å‡½æ•°ä¸‹è½½æ¨¡å‹ï¼Œç¬¬ä¸€ä¸ªå‚æ•°ä¸ºæ¨¡å‹åç§°ï¼Œå‚æ•° `cache_dir`ä¸ºæ¨¡å‹çš„ä¸‹è½½è·¯å¾„ã€‚

ç„¶åæ–°å»ºåä¸º `model_download.py` çš„ `python` è„šæœ¬ï¼Œå¹¶åœ¨å…¶ä¸­è¾“å…¥ä»¥ä¸‹å†…å®¹å¹¶ä¿å­˜

```python
# model_download.py
import os
import torch
from modelscope import snapshot_download, AutoModel, AutoTokenizer
model_dir = snapshot_download('qwen/Qwen2-7B-Instruct', cache_dir='', revision='master')
```

ç„¶ååœ¨ç»ˆç«¯ä¸­è¾“å…¥ `python model_download.py` æ‰§è¡Œä¸‹è½½ï¼Œè¿™é‡Œéœ€è¦è€å¿ƒç­‰å¾…ä¸€æ®µæ—¶é—´ç›´åˆ°æ¨¡å‹ä¸‹è½½å®Œæˆã€‚

#### 3.2.2.3 ä»£ç å‡†å¤‡
 **Pythonè„šæœ¬**

åœ¨ `/root/autodl-tmp` è·¯å¾„ä¸‹æ–°å»º `vllm_model.py` æ–‡ä»¶å¹¶åœ¨å…¶ä¸­è¾“å…¥ä»¥ä¸‹å†…å®¹ã€‚

é¦–å…ˆä» `vLLM` åº“ä¸­å¯¼å…¥ `LLM` å’Œ `SamplingParams` ç±»ã€‚`LLM` ç±»æ˜¯ä½¿ç”¨ `vLLM` å¼•æ“è¿è¡Œç¦»çº¿æ¨ç†çš„ä¸»è¦ç±»ã€‚`SamplingParams` ç±»æŒ‡å®šé‡‡æ ·è¿‡ç¨‹çš„å‚æ•°ï¼Œç”¨äºæ§åˆ¶å’Œè°ƒæ•´ç”Ÿæˆæ–‡æœ¬çš„éšæœºæ€§å’Œå¤šæ ·æ€§ã€‚

`vLLM` æä¾›äº†éå¸¸æ–¹ä¾¿çš„å°è£…ï¼Œæˆ‘ä»¬ç›´æ¥ä¼ å…¥æ¨¡å‹åç§°æˆ–æ¨¡å‹è·¯å¾„å³å¯ï¼Œä¸å¿…æ‰‹åŠ¨åˆå§‹åŒ–æ¨¡å‹å’Œåˆ†è¯å™¨ã€‚

æˆ‘ä»¬å¯ä»¥é€šè¿‡è¿™ä¸ªä»£ç ç¤ºä¾‹ç†Ÿæ‚‰ä¸‹ ` vLLM` å¼•æ“çš„ä½¿ç”¨æ–¹å¼ã€‚

```python
# vllm_model.py
from vllm import LLM, SamplingParams
from transformers import AutoTokenizer
import os
import json

# è‡ªåŠ¨ä¸‹è½½æ¨¡å‹æ—¶ï¼ŒæŒ‡å®šä½¿ç”¨modelscopeã€‚ä¸è®¾ç½®çš„è¯ï¼Œä¼šä» huggingface ä¸‹è½½
os.environ['VLLM_USE_MODELSCOPE']='True'

def get_completion(prompts, model, tokenizer=None, max_tokens=512, temperature=0.8, top_p=0.95, max_model_len=2048):
    stop_token_ids = [151329, 151336, 151338]
    # åˆ›å»ºé‡‡æ ·å‚æ•°ã€‚temperature æ§åˆ¶ç”Ÿæˆæ–‡æœ¬çš„å¤šæ ·æ€§ï¼Œtop_p æ§åˆ¶æ ¸å¿ƒé‡‡æ ·çš„æ¦‚ç‡
    sampling_params = SamplingParams(temperature=temperature, top_p=top_p, max_tokens=max_tokens, stop_token_ids=stop_token_ids)
    # åˆå§‹åŒ– vLLM æ¨ç†å¼•æ“
    llm = LLM(model=model, tokenizer=tokenizer, max_model_len=max_model_len,trust_remote_code=True)
    outputs = llm.generate(prompts, sampling_params)
    return outputs


if __name__ == "__main__":  
    # åˆå§‹åŒ– vLLM æ¨ç†å¼•æ“
    model='/root/autodl-tmp/qwen/Qwen2-7B-Instruct' # æŒ‡å®šæ¨¡å‹è·¯å¾„
    # model="qwen/Qwen2-7B-Instruct" # æŒ‡å®šæ¨¡å‹åç§°ï¼Œè‡ªåŠ¨ä¸‹è½½æ¨¡å‹
    tokenizer = None
    # åŠ è½½åˆ†è¯å™¨åä¼ å…¥vLLM æ¨¡å‹ï¼Œä½†ä¸æ˜¯å¿…è¦çš„ã€‚
    # tokenizer = AutoTokenizer.from_pretrained(model, use_fast=False) 
  
    text = ["ä½ å¥½ï¼Œå¸®æˆ‘ä»‹ç»ä¸€ä¸‹ä»€ä¹ˆæ—¶å¤§è¯­è¨€æ¨¡å‹ã€‚",
            "å¯ä»¥ç»™æˆ‘å°†ä¸€ä¸ªæœ‰è¶£çš„ç«¥è¯æ•…äº‹å—ï¼Ÿ"]
    # messages = [
    #     {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„åŠ©æ‰‹ã€‚"},
    #     {"role": "user", "content": prompt}
    # ]
    # ä½œä¸ºèŠå¤©æ¨¡æ¿çš„æ¶ˆæ¯ï¼Œä¸æ˜¯å¿…è¦çš„ã€‚
    # text = tokenizer.apply_chat_template(
    #     messages,
    #     tokenize=False,
    #     add_generation_prompt=True
    # )

    outputs = get_completion(text, model, tokenizer=tokenizer, max_tokens=512, temperature=1, top_p=1, max_model_len=2048)

    # è¾“å‡ºæ˜¯ä¸€ä¸ªåŒ…å« promptã€ç”Ÿæˆæ–‡æœ¬å’Œå…¶ä»–ä¿¡æ¯çš„ RequestOutput å¯¹è±¡åˆ—è¡¨ã€‚
    # æ‰“å°è¾“å‡ºã€‚
    for output in outputs:
        prompt = output.prompt
        generated_text = output.outputs[0].text
        print(f"Prompt: {prompt!r}, Generated text: {generated_text!r}")
```

è¿è¡Œä»£ç 

```bash
cd /root/autodl-tmp && python vllm_model.py
```

ç»“æœå¦‚ä¸‹ï¼š

```bash
Prompt: 'ä½ å¥½ï¼Œå¸®æˆ‘ä»‹ç»ä¸€ä¸‹ä»€ä¹ˆæ—¶å¤§è¯­è¨€æ¨¡å‹ã€‚', Generated text: ' å½“ç„¶ï¼å¤§è¯­è¨€æ¨¡å‹æ˜¯äººå·¥æ™ºèƒ½ä¸­çš„ä¸€ç§æ¨¡å‹ï¼Œç‰¹åˆ«æ“…é•¿ç”Ÿæˆé«˜è´¨é‡çš„æ–‡æœ¬ã€‚å®ƒä»¬ä»å¤§é‡çš„æ–‡æœ¬æ•°æ®ä¸­å­¦ä¹ ï¼Œå¹¶å¯ä»¥ç”Ÿæˆç±»ä¼¼çœŸå® æ–‡æœ¬çš„æ–‡æœ¬ç‰‡æ®µã€‚ä¾‹å¦‚ï¼Œè®©å®ƒä»¬å†™æ•…äº‹ã€æ–‡ç« ã€è¯—æ­Œï¼Œæˆ–è€…åœ¨å¯¹è¯ä¸­ç”Ÿæˆè¿è´¯çš„å›ç­”ã€‚è¿™ç±»æ¨¡å‹ä¹Ÿè¢«ç”¨äºè®¸å¤šå…¶ä»–è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ï¼Œå¦‚æ–‡æœ¬æ‘˜è¦ã€ç¿»è¯‘å’Œä»£ç ç”Ÿæˆã€‚è¿™æ˜¯å› ä¸ºå®ƒä»¬èƒ½å¤Ÿç†è§£å’Œç”Ÿæˆå¤æ‚çš„ è¯­æ³•å’Œè¯­ä¹‰ç»“æ„ï¼Œä»¥åŠæ•æ‰åˆ°ä¸Šä¸‹æ–‡ä¸­çš„å¾®å°ç»†èŠ‚ã€‚å¤§è¯­è¨€æ¨¡å‹çš„æ ¸å¿ƒæ˜¯é‡‡ç”¨æ·±åº¦å­¦ä¹ æŠ€æœ¯ï¼Œå°¤å…¶æ˜¯åŸºäºTransformeræ¶æ„çš„æ¨¡å‹ï¼Œè¿™ç§æ¶æ„å¾ˆå¥½åœ°å¤„ç†äº†å¤§é‡çš„åºåˆ—æ•°æ®ï¼Œå¹¶åœ¨æœ€è¿‘å‡ å¹´å–å¾—äº†æ˜¾è‘—çš„è¿›å±•ï¼Œè¿™å¾—ç›Šäºå¤§è§„æ¨¡çš„è®­ç»ƒæ•°æ®é›†å’Œè®¡ç®—èµ„æºã€‚å¦‚ä»Šï¼Œè®¸å¤šå¤§å‹è¯­è¨€æ¨¡å‹æ˜¯å¼€æºçš„ï¼Œå¹¶ä¸”åº”ç”¨äºå„ç§å¼€å‘å’Œç ”ç©¶ç¯å¢ƒä¸­ã€‚'

Prompt: 'å¯ä»¥ç»™æˆ‘å°†ä¸€ä¸ªæœ‰è¶£çš„ç«¥è¯æ•…äº‹å—ï¼Ÿ', Generated text: ' å½“ç„¶å¯ä»¥ã€‚è¿™æ˜¯ä¸€ä¸ªå…³äºå‹‡æ•¢çš„å°çŒ«å¤´é¹°çš„ä¸»é¢˜çš„ç«¥è¯æ•…äº‹ï¼š\n\nä»å‰ï¼Œåœ¨ä¸€ç‰‡å®é™çš„æ£®æ—æ·±å¤„ï¼Œä½ç€ä¸€ä¸ªèªæ˜è€Œå‹‡æ•¢çš„å°çŒ«å¤´é¹°ã€‚ å®ƒçš„åå­—å«è¿ˆå…‹ã€‚æ¯å¤©ï¼Œå®ƒéƒ½ä¼šåœ¨å¤œè‰²ä¸­ç©¿æ¢­ï¼Œå¯»æ‰¾é£Ÿç‰©å’Œå­¦ä¹ æ›´å¤šçš„ç”Ÿå­˜ä¹‹é“ã€‚å®ƒçš„å®¶æ˜¯ä¸€ä¸ªå®ƒè‡ªå·±åˆ¶ä½œçš„å·¨å¤§é¸Ÿå·¢ï¼ŒæŒ‚åœ¨ä¸€ç‰‡æ¾æ ‘çš„é«˜æä¸Šã€‚\n\nä¸€å¤©å¤œé‡Œï¼Œæ£®æ—å—åˆ°äº†å¨èƒï¼Œå› ä¸ºä¸€åªè´ªå©ªçš„è€æ¯ç‹¼ å›é¢†äº†ä¸€é˜Ÿå¼ºç›—åœ¨ä»–çš„é¢†åœ°æ‰“åŠ«ã€‚æ‰€æœ‰åŠ¨ç‰©éƒ½é™·å…¥äº†ææ…Œï¼Œèƒ†å°çš„ä»¬èº²åœ¨å®¶é‡Œä¸æ•¢å‡ºæ¥ï¼Œè€Œèƒ†å¤§çš„åŠ¨ç‰©ä»¬åˆ™æ˜¯å››å¤„é€ƒéš¾ã€‚ä½†æ˜¯ï¼Œæ²¡æœ‰ä¸€åªåŠ¨ç‰©æ•¢äºæŒ‘æˆ˜æ¯ç‹¼ã€‚\n\nä½œä¸ºå‹‡æ•¢å’Œæ™ºæ…§çš„è±¡å¾ï¼Œå°çŒ«å¤´é¹°è¿ˆå…‹å†³ å®šæŒºèº«è€Œå‡ºã€‚å®ƒè®¤è¯†åˆ°å•é é‡å…½çš„åŠ›é‡æ˜¯æ— æ³•å¯¹æŠ—æ¯ç‹¼åŠå…¶éšä»çš„ï¼Œä½†æ˜¯å‡­å€Ÿæ™ºæ…§ä¸ç­–ç•¥ï¼Œå®ƒæˆ–è®¸å¯ä»¥æ‰¾åˆ°ä¸€æ¡è§£å†³æ–¹æ¡ˆã€‚\n\nä¸æ—¥ï¼Œè¿ˆå…‹å¸¦ç€ä¸€ä¸ªå¤§èƒ†çš„è®¡åˆ’å›åˆ°äº†æ£®æ—ã€‚å®ƒå®£å¸ƒï¼Œæ‰€æœ‰çš„ç”Ÿç‰©éƒ½å°†æš‚æ—¶ æ”¾ä¸‹å½¼æ­¤ä¹‹é—´çš„äº‰æ–—ï¼Œæºæ‰‹åˆä½œå¯¹æŠ—è¿™åœºå±æœºã€‚ä¸ºäº†åšåˆ°è¿™ä¸€ç‚¹ï¼Œè¿ˆå…‹å°†åŠ¨ç‰©ä»¬èšé›†åœ¨ä¸€èµ·ï¼Œè®©è¿·äººçš„åŠ¨ç‰©å­¦è€…ç™½é¹¤æ•™æˆæ•™æˆæ‰€æœ‰ç”Ÿç‰©å¦‚ä½•å½¼æ­¤æ²Ÿé€šã€ç†è§£ï¼Œå¹¶åŠ¨å‘˜å„å…·ä¸“ä¸šèƒ½åŠ›çš„åŠ¨ç‰©ï¼Œå¦‚æŒ–æ˜ä¸“å®¶è€é¼  ã€ç”µå­è®¾å¤‡ä¸“å®¶æ¾é¼ åˆ¶ä½œæ— çº¿ç”µæ¥ç§˜å¯†å‘æ£®æ—é‡Œçš„å…¶ä»–åŠ¨ç‰©å‘é€æ±‚åŠ©ä¿¡æ¯ã€‚\n\nè®¡åˆ’é€æ¸å±•å¼€ï¼ŒåŠ¨ç‰©ä»¬å¼€å§‹æœ‰äº†é˜²èŒƒæ„è¯†ï¼Œå¹¶åœ¨å¤œæ™šéªšåŠ¨çš„å¥³ç‹¼ç¾¤ä¸çŸ¥é“ä»»ä½•äººè®¡åˆ’çš„æ—¶å€™åšå‡ºäº†å„ç§æœ‰æ•ˆçš„é˜²å¾¡ã€‚åŠ¨ç‰©ä¸­ ä¸ªä¸ªéƒ½è´¡çŒ®äº†ä»–ä»¬çš„åŠ›é‡ã€‚å…”å­ä¸è²˜å µä½äº†å‡ ä¸ªé‡è¦çš„å…¥å£ï¼Œçµå·§çš„æ¾é¼ ä»¬åˆ™æ”¶é›†äº†å¤§é‡çš„æµ†æœå’Œè¥å…»ç‰©è´¨ï¼Œä»¥ä¾›æ•´ä¸ªæ£®æ—çš„åŠ¨ç‰©ä»¬è¡¥å……èƒ½é‡ã€‚\n\næœ€åï¼Œåœ¨ä¸€åœºå¤œé‡Œçš„æ˜æ™ºé€®æ•è¡ŒåŠ¨ä¹‹åï¼Œè¿ˆå…‹çš„å°çŒ«å¤´ é¹°å·§å¦™åœ°é€šè¿‡å…¶è¾ƒå¥½çš„å¤œè§†å’Œå¬åŠ›ï¼Œè”åˆç³ç†Šå’Œç‹®å­æˆåŠŸçš„å°†è´ªå©ªçš„è€æ¯ç‹¼åŠå…¶å…±çŠ¯èµ¶å‡ºäº†æ£®æ—ã€‚\n\næ¶ˆæ¯éä¼ ï¼Œæ‰€æœ‰åŠ¨ç‰©éƒ½å¯¹å°çŒ«å¤´é¹°çš„æ™ºæ…§ï¼Œå‹‡æ•¢ä»¥åŠä½œä¸ºå›¢é˜Ÿé¢†è¢–çš„åŠ›é‡è¡¨ç¤ºäº†æ•¬æ„ã€‚ä»–ä»¬ç°åœ¨ç´§ç´§ å›¢ç»“åœ¨äº†ä¸€èµ·ï¼Œå»ºç«‹äº†å’Œè°è€Œæœ‰å°Šä¸¥çš„ç¤¾ç¾¤ã€‚\n\nä»æ­¤ï¼Œæ£®æ—ä¸­å……æ»¡äº†æ¬¢å£°ç¬‘è¯­ï¼ŒåŠ¨ç‰©ä»¬å’Œå°çŒ«å¤´é¹°è¿ˆå…‹ä¸€èµ·å¿«ä¹åœ°ç”Ÿæ´»åœ¨å’Œå¹³ä¸å’Œè°ä¸­ï¼Œå±•ç°å‡ºå›¢ç»“ä¸æ™ºæ…§çš„ä¼Ÿå¤§åŠ›é‡ã€‚è¿™åˆ™æ•…äº‹æ•™ä¼šæˆ‘ä»¬ï¼Œå½“æˆ‘ä»¬å›¢ç»“ ä¸€è‡´ï¼Œæ•¢äºé¢å¯¹å›°éš¾ï¼Œå‘æŒ¥åˆ›é€ åŠ›å’Œå…±åŒåŠªåŠ›æ—¶ï¼Œæ²¡æœ‰ä»€ä¹ˆä¸å¯èƒ½å…‹æœçš„ã€‚'
```